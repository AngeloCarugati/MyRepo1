{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5bb2981e-1bb3-409b-be69-cf2b1363c061",
   "metadata": {},
   "source": [
    "# Corpus File Preprocessing for Word2Vec:\n",
    "## 1. Load and read the corpus.\n",
    "## 2. Tokenize the text.\n",
    "## 3. Lowercase all words.\n",
    "## 4. Remove stop words and non-alphabetic words.\n",
    "## 5. (Optional) Lemmatize words.\n",
    "## 6. Optionally, subsample frequent words.\n",
    "## 7. Prepare the corpus for Word2Vec (list of tokenized sentences).\n",
    "## 8. Train the Word2Vec model using Gensim or other frameworks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5ec895",
   "metadata": {},
   "source": [
    "# PARTE 1: Costruzione del Modello Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef8a156e",
   "metadata": {},
   "source": [
    "### Import delle librerie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ebb78b5-e7f4-42ce-ba79-518a749e317d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('punkt_tab')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import string\n",
    "\n",
    "import spacy\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc71f3d7",
   "metadata": {},
   "source": [
    "### Lettura e tokenizzazione per frasi del Corpus documentale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed1d4f8-143e-4355-894e-7f23707a0ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r\"/home/acarugat/MyRepo1/Testi/Manzoni/Corpus.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    corpus = file.read()\n",
    "corpus=sent_tokenize(corpus, language='italian')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc17efe",
   "metadata": {},
   "source": [
    "### Tokenizzazione per parole del Corpus documentale togliendo la punteggiatura ... e non solo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86343135-939a-4c3b-b913-949f1784fba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_corpus = [word_tokenize(sentence.lower(), language='italian') for sentence in corpus]\n",
    "\n",
    "punteggiatura = string.punctuation+\"«»’\"\n",
    "\n",
    "tokenized_corpus2=[]\n",
    "for sentence in tokenized_corpus:\n",
    "    sentence2=[]\n",
    "    for word in sentence:\n",
    "        if word not in punteggiatura:\n",
    "            sentence2.append(word)\n",
    "    tokenized_corpus2.append(sentence2)\n",
    "    \n",
    "tokenized_corpus=tokenized_corpus2   \n",
    "    \n",
    "#okenized_corpus = [word for word in tokenized_corpus if word not in string.punctuation]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13edf8c3",
   "metadata": {},
   "source": [
    "### Togliamo gli apostrofi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90fe753",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_corpus_noap = []\n",
    "for sottolista in tokenized_corpus:\n",
    "    nuova_sottolista = []\n",
    "    for elemento in sottolista:\n",
    "        if \"'\" in elemento:  # Se l'elemento contiene un apostrofo\n",
    "            nuova_sottolista.extend(elemento.split(\"'\"))  # Splitta e aggiungi le parti\n",
    "        else:\n",
    "            nuova_sottolista.append(elemento)  # Aggiungi direttamente l'elemento\n",
    "    tokenized_corpus_noap.append(nuova_sottolista)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f47bfa",
   "metadata": {},
   "source": [
    "### Togliamo le stopwords ... e non solo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f4701f-b473-406a-8e24-945a2f9d963a",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('italian'))\n",
    "stop_words.update([\"d\", \"gl\", \"de\", \"et\", \"s\", \"o\", \"...\"])\n",
    "cleaned_corpus = [[word for word in sentence if word not in stop_words] for sentence in tokenized_corpus_noap]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a161ba1",
   "metadata": {},
   "source": [
    "### Salviamo il Corpus documentale preprocessato"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89223425-c8ab-430a-978f-f7dad9974feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r\"/home/acarugat/MyRepo1/Testi/Manzoni/Corpus-PP.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "    for sentence in cleaned_corpus:\n",
    "        file.write(\" \".join(sentence) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa36183f",
   "metadata": {},
   "source": [
    "### Generiamo il Modello Word2Vec a partire dal Corpus documentale preprocessato"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6534d80-3430-4fc3-967f-84d0e1fdc3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the Word2Vec model\n",
    "model = Word2Vec(corpus_file=r\"/home/acarugat/MyRepo1/Testi/Manzoni/Corpus-PP.txt\", vector_size=100, window=5, min_count=5, workers=4)\n",
    "\n",
    "# Save the trained model\n",
    "model.save(\"word2vec_model.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11dc3e63",
   "metadata": {},
   "source": [
    "### Valutiamo il Modello ottenuto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60bcd48-19a4-4ffa-a5ef-eef740f13e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (model.wv.get_vecattr(\"lago\", \"count\"))\n",
    "print (model.wv.most_similar(\"lago\"))\n",
    "print (model.wv.similarity(\"lago\", \"fiume\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7adf99f4",
   "metadata": {},
   "source": [
    "### Addestriamo il Modello Word2Vec model con più iterazioni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b8f6e4-2c1e-4351-bdc5-68ab77c9fcd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(corpus_file=r\"/home/acarugat/MyRepo1/Testi/Manzoni/Corpus-PP.txt\", vector_size=100, window=5, min_count=5, workers=4, epochs=100, sg=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10da7887",
   "metadata": {},
   "source": [
    "### Valutiamo il Modello ottenuto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0421d541-ede2-46c5-82d4-2845490f23af",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (model.wv.get_vecattr(\"lago\", \"count\"))\n",
    "print (model.wv.most_similar(\"lago\"))\n",
    "print (model.wv.similarity(\"lago\", \"fiume\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f63d2515",
   "metadata": {},
   "source": [
    "### Addestriamo il Modello Word2Vec model con più iterazioni e algoritmo \"skip-grammar\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f0c885-2d68-4e69-851c-6e26d2d26098",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(corpus_file=r\"/home/acarugat/MyRepo1/Testi/Manzoni/Corpus-PP.txt\", vector_size=100, window=5, min_count=5, workers=4, epochs=100, sg=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbdd73f4",
   "metadata": {},
   "source": [
    "### Valutiamo il Modello ottenuto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ef2fbe-c74a-4856-b1fa-6e2e86b9b876",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (model.wv.get_vecattr(\"lago\", \"count\"))\n",
    "print (model.wv.most_similar(\"lago\"))\n",
    "print (model.wv.similarity(\"lago\", \"fiume\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71df592a",
   "metadata": {},
   "source": [
    "# PARTE 1: Analisi della Frequenza delle Parole"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c17776fc",
   "metadata": {},
   "source": [
    "### Otteniamo la lista delle parole presenti nel Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd17763-4df1-4ca9-8ee6-14efcfcc1198",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "\n",
    "# Flatten the list\n",
    "flattened_corpus = list(chain.from_iterable(cleaned_corpus))\n",
    "\n",
    "# Get unique tokens\n",
    "parole_presenti = list(set(flattened_corpus))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f59748f",
   "metadata": {},
   "source": [
    "### Otteniamo la lista delle parole presenti nel Modello (teniamo presente che min_count = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13476db2-0d40-4d8c-9802-213ecd1b41e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#In Gensim's Word2Vec, the vocabulary already contains unique words\n",
    "tokens_unici= list(model.wv.index_to_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b160de6b",
   "metadata": {},
   "source": [
    "## Per ogni parola presente nel Modello otteniamo la frequenza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92689df9-5864-44cb-873d-8db30493254f",
   "metadata": {},
   "outputs": [],
   "source": [
    "    tabella = []\n",
    "    for parola in tokens_unici:\n",
    "        c=model.wv.get_vecattr(parola,\"count\")\n",
    "        tabella.append({\"Parola\":parola, \"Frequenza\":c})\n",
    "    df=pd.DataFrame(tabella)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6becd47",
   "metadata": {},
   "source": [
    "### Stampiamo il grafico delle frequenze delle parole con nltk.FreqDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c89f602-8fac-477f-ae4b-453a1eec5d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_dist=nltk.FreqDist(flattened_corpus)\n",
    "freq_dist.plot(10, cumulative=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564f429c",
   "metadata": {},
   "source": [
    "### Stampiamo il grafico delle frequenze delle parole con seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47691f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imposta lo stile di Seaborn (opzionale)\n",
    "# sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Crea il grafico a barre\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='Frequenza', y='Parola', data=df.sort_values('Frequenza', ascending=False).head(10))\n",
    "\n",
    "# Aggiungi titolo e etichette agli assi\n",
    "plt.title('Le 10 parole più frequenti', fontsize=16)\n",
    "plt.xlabel('Frequenza', fontsize=12)\n",
    "plt.ylabel('Parola', fontsize=12)\n",
    "\n",
    "# Mostra il grafico\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed66097",
   "metadata": {},
   "source": [
    "# PARTE 3: CLUSTERIZZAZIONE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af4a42a",
   "metadata": {},
   "source": [
    "### Generazione dei Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "febb7484",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esempio: vettori = [model[word] for word in parole_presenti]\n",
    "vettori = [model.wv[word] for word in tokens_unici]\n",
    "           \n",
    "# Converti la lista in un array NumPy\n",
    "vettori_array = np.array(vettori)\n",
    "\n",
    "# Applica t-SNE\n",
    "# t-SNE (t-distributed Stochastic Neighbor Embedding) \n",
    "# è una tecnica di riduzione della dimensione utilizzata per visualizzare dati ad alta dimensione \n",
    "# in uno spazio a bassa dimensione (tipicamente 2D o 3D)\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "# Generiamo un \"modello\" ridotto  a 2 dimensioni\n",
    "vettori_ridotti = tsne.fit_transform(vettori_array)\n",
    "\n",
    "# Visualizza i risultati\n",
    "#plt.scatter(vettori_ridotti[:, 0], vettori_ridotti[:, 1])\n",
    "#for i, parola in enumerate(tokens_unici):\n",
    "#    plt.text(vettori_ridotti[i, 0] + 0.01, vettori_ridotti[i, 1] + 0.01, parola, fontsize=9)\n",
    "#plt.title(\"Visualizzazione t-SNE\")\n",
    "#plt.show()\n",
    "\n",
    "\n",
    "# Imposta il numero di cluster\n",
    "num_clusters = 100\n",
    "\n",
    "# Esegui il clustering\n",
    "#kmeans = KMeans(n_clusters=num_clusters, n_init='auto', random_state=42)\n",
    "'''\n",
    "La funzione KMeans è un algoritmo di clustering (o raggruppamento) non supervisionato \n",
    "che viene utilizzato per suddividere un dataset in un numero specificato di gruppi o cluster. \n",
    "Ogni gruppo rappresenta un insieme di punti dati che sono \"simili\" tra loro, \n",
    "basandosi su una misura di distanza (tipicamente la distanza euclidea).\n",
    "'''\n",
    "#kmeans = KMeans(n_clusters=num_clusters, init='k-means++', n_init='auto', max_iter=300, tol=0.0001, verbose=0, random_state=None, copy_x=True, algorithm='lloyd')\n",
    "kmeans = KMeans(n_clusters=num_clusters, init='k-means++', n_init=100, max_iter=300, tol=0.0001, verbose=0, random_state=None, copy_x=True, algorithm='lloyd')\n",
    "\n",
    "kmeans.fit(vettori)\n",
    "\n",
    "# Ottieni i cluster\n",
    "'''\n",
    " l'attributo labels_ contiene un array di etichette, una per ogni punto nel dataset di input, \n",
    " che indica a quale cluster ogni punto è stato assegnato.\n",
    "'''\n",
    "clusters = kmeans.labels_\n",
    "\n",
    "# Raggruppa le parole per cluster\n",
    "cluster_parole = {i: [] for i in range(num_clusters)}\n",
    "for parola, cluster in zip(tokens_unici, clusters):\n",
    "    cluster_parole[cluster].append(parola)\n",
    "\n",
    "# Visualizza i cluster\n",
    "for cluster, parole in cluster_parole.items():\n",
    "    print(f\"Cluster {cluster}: {parole}\")\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    for i, parola in enumerate(parole):\n",
    "        plt.scatter(vettori_ridotti[i, 0], vettori_ridotti[i, 1])\n",
    "        plt.annotate(parola, (vettori_ridotti[i, 0], vettori_ridotti[i, 1]))\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0dcff1",
   "metadata": {},
   "source": [
    "# PARTE 4: Analisi delle similarità lessicali"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "470bfc6d",
   "metadata": {},
   "source": [
    "### Similarità tra due documenti tramite sklearn.feature_extraction.text.TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5198aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "La matrice di similarità\n",
    "\n",
    "La matrice risultante da tfidf * tfidf.T è una matrice di dimensioni 2x2 \n",
    "(poiché stiamo confrontando solo due documenti, a e b). \n",
    "Il prodotto scalare tra i due vettori TF-IDF è calcolato per ciascun paio di documenti. \n",
    "La matrice risultante avrà la seguente struttura:\n",
    "Matrice di similaritaˋ=(sim(a,a)sim(a,b)sim(b,a)sim(b,b))\n",
    "Matrice di similaritaˋ=(sim(a,a)sim(b,a)​sim(a,b)sim(b,b)​)\n",
    "\n",
    "Dove:\n",
    "\n",
    "    sim(a, a) è la similarità coseno tra il documento a e se stesso. \n",
    "    Questo valore sarà sempre 1, perché ogni vettore è perfettamente simile a se stesso.\n",
    "    sim(b, b) è la similarità coseno tra il documento b e se stesso. \n",
    "    Anche questo valore sarà 1.\n",
    "    sim(a, b) è la similarità coseno tra i due documenti, a e b.\n",
    "    sim(b, a) è la stessa cosa di sim(a, b), poiché la similarità coseno è simmetrica.\n",
    "'''\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "def calcola_similarita(a, b): \n",
    "    tfidf = vectorizer.fit_transform([a, b])\n",
    "    #print (\"Colonna 0, Riga 0:\", ((tfidf * tfidf.T).toarray())[0,0])\n",
    "    #print (\"Colonna 0, Riga 1:\", ((tfidf * tfidf.T).toarray())[0,1])\n",
    "    #print (\"Colonna 1, Riga 0:\", ((tfidf * tfidf.T).toarray())[1,0])\n",
    "    #print (\"Colonna 1, Riga 1:\", ((tfidf * tfidf.T).toarray())[1,1])\n",
    "    return ((tfidf * tfidf.T).toarray())[0,1]\n",
    "\n",
    "file1_path = r\"/home/acarugat/MyRepo1/Testi/Manzoni/Fermo-e-Lucia.txt\"\n",
    "\n",
    "with open(file1_path, 'r', encoding='utf-8') as file: \n",
    "    testo1 = file.read()\n",
    "\n",
    "file2_path = r\"/home/acarugat/MyRepo1/Testi/Manzoni/PromessiSposi-1840.txt\"\n",
    "\n",
    "with open(file2_path, 'r', encoding='utf-8') as file: \n",
    "    testo2 = file.read()\n",
    "\n",
    "s=calcola_similarita(testo1,testo2)\n",
    "\n",
    "print (s)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ba5e8b",
   "metadata": {},
   "source": [
    "### Similarità tra insiemi di documenti: Per ogni file presente in una directory specificata in input,\n",
    "### metti in un dataframe: il nome del file e il contenuto (testo) del documento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04701dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to store filenames and contents\n",
    "filenames = []\n",
    "contents = []\n",
    "\n",
    "# Walk through the home directory and read all text files\n",
    "print (\"Dimmi che documenti vuoi analizzare: Manzoni Presidenti Giornali o Parlamento\")\n",
    "line = input()\n",
    "\n",
    "for root, dirs, files in os.walk(r\"/home/acarugat/MyRepo1/Testi/\"+line):\n",
    "    for file in files:\n",
    "        if file not in [\"Corpus.txt\", \"Corpus-PP.txt\", \"PromessiSposi-stopword.txt\", \"MattarellaFine2024-preprocessato.txt\"]:\n",
    "            if file.endswith(\".txt\"):  # Check if the file is a text file\n",
    "                filepath = os.path.join(root, file)\n",
    "                try:\n",
    "                    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "                        filenames.append(filepath)  # Store the file path\n",
    "                        contents.append(f.read())  # Store the file content\n",
    "                except Exception as e:\n",
    "                    print(f\"Error reading {filepath}: {e}\")\n",
    "\n",
    "# Create the dataset (DataFrame)\n",
    "dataset = pd.DataFrame({\n",
    "    \"filename\": filenames,\n",
    "    \"content\": contents\n",
    "})\n",
    "\n",
    "# Display the dataset\n",
    "# print(dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c47f5c32",
   "metadata": {},
   "source": [
    "### creiamo una matrice M per contenere le similarità di testo_i con testo_j usando tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc86c1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "tqdm è una libreria Python che fornisce una barra di progresso \n",
    "facile da usare e visivamente attraente per monitorare il progresso di loop, operazioni \n",
    "o processi che richiedono tempo. \n",
    "Il nome tqdm sta per \"taqaddum\", che significa \"progresso\" in arabo.\n",
    "'''\n",
    "\n",
    "M = np.zeros((dataset.shape[0], dataset.shape[0])) \n",
    "#N.B.: essendo la matrice M quadrata, dataset.shape[0] = numero di righe = dataset.shape[1] = numero di colonne \n",
    "for i, row in tqdm(dataset.iterrows(), total=dataset.shape[0], desc='1st level'): # definiamo i\n",
    "    for j, next_row in dataset.iterrows(): # definiamo j\n",
    "        M[i, j] = calcola_similarita(row.content, next_row.content) # popoliamo la matrice con i risultati"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a78a9a",
   "metadata": {},
   "source": [
    "### Creiamo un DataFrame contenente le similarità"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43970f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels=dataset.filename.str.split('/').str[5:].str[1]\n",
    "labels=labels.str.split('.').str[0]\n",
    "similarity_df = pd.DataFrame(M, columns=labels, index=labels) # creiamo un dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441f4178",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Rappresentazione grafica mediante heatmap delle similarità tra diversi documenti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040fa322",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creiamo la visualizzazione\n",
    "plt.figure(figsize=(12, 12))\n",
    "sns.heatmap(\n",
    "\t\t\tsimilarity_df,\n",
    "\t\t\tsquare=True, \n",
    "\t\t\tannot=True, \n",
    "\t\t\trobust=True,\n",
    "\t\t\tfmt='.2f',\n",
    "\t\t\tannot_kws={'size': 7, 'fontweight': 'bold'},\n",
    "\t\t\tyticklabels=similarity_df.columns,\n",
    "\t\t\txticklabels=similarity_df.columns,\n",
    "\t\t\tcmap=\"YlGnBu\",\n",
    "            #mask=mask\n",
    "\t\t\tmask=None\n",
    ")\n",
    "\n",
    "plt.title('Heatmap delle similarità tra testi', fontdict={'fontsize': 24})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5945405f",
   "metadata": {},
   "source": [
    "# PARTE 5: Che Parole Usi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d091ec",
   "metadata": {},
   "source": [
    "### Carichiamo Modello Italiano di Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ad727c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carica il modello di lingua italiana di spaCy\n",
    "nlp = spacy.load(\"it_core_news_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e14820",
   "metadata": {},
   "source": [
    "### Definiamo funzione \"trova_lessemi\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feecdbbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.max_length = 10000000\n",
    "# Funzione per estrarre i lessemi (lemmi) da un testo, escludendo le stop words\n",
    "def trova_lessemi_da_file(nome_file):\n",
    "    try:\n",
    "        # Leggi il contenuto del file\n",
    "        with open(nome_file, \"r\", encoding=\"utf-8\") as file:\n",
    "            testo = file.read()\n",
    "        \n",
    "        # Analizza il testo\n",
    "        doc = nlp(testo)\n",
    "        \n",
    "        # Estrai i lessemi, escludendo punteggiatura, spazi e stop words\n",
    "        lessemi = [\n",
    "            token.lemma_\n",
    "            for token in doc\n",
    "            if not token.is_punct and not token.is_space and not token.is_stop and not token.pos_ == \"DET\" and not token.pos_ == \"ADP\"\n",
    "        ]\n",
    "        return lessemi\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Errore: Il file '{nome_file}' non è stato trovato.\")\n",
    "        return []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbdc58be",
   "metadata": {},
   "source": [
    "### Costruiamo Lessico di Base da File di Input contenente Lista di Parole Italiane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110364bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/home/acarugat/paroleitaliane.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    lessicobase = file.read().lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce719db4",
   "metadata": {},
   "source": [
    "### Definiamo Funzione che Rimuove Accenti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d51d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "\n",
    "# Funzione per rimuovere gli accenti\n",
    "def rimuovi_accenti(testo):\n",
    "    testo_normalizzato = unicodedata.normalize(\"NFD\", testo)\n",
    "    testo_senza_accenti = \"\".join(char for char in testo_normalizzato if not unicodedata.combining(char))\n",
    "    return testo_senza_accenti"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad05812",
   "metadata": {},
   "source": [
    "### Confrontiamo il Lessico del Documento da analizzare con il Lessico di Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2273be86",
   "metadata": {},
   "outputs": [],
   "source": [
    "parole_presenti = []\n",
    "parole_assenti = []\n",
    "# Specifica il nome del file di input\n",
    "nome_file = input()\n",
    "with open(nome_file, \"r\", encoding=\"utf-8\") as file:\n",
    "    testo = file.read().lower()\n",
    "    \n",
    "    # Passa il testo alla pipeline di spaCy\n",
    "doc = nlp(testo)\n",
    "\n",
    "# Esegui alcune analisi\n",
    "for parola in doc:\n",
    "    if not parola.is_punct and not parola.is_space and not parola.is_stop and not parola.pos_ == \"DET\" and not parola.pos_ == \"ADP\":\n",
    "        print(f\"Token: {parola.text}, POS: {parola.pos_}, Lemma: {parola.lemma_}\")\n",
    "        lemma_senza_accenti = rimuovi_accenti (parola.lemma_)\n",
    "        lemma_senza_accenti_minuscolo = lemma_senza_accenti.lower()\n",
    "        if lemma_senza_accenti_minuscolo in lessicobase:\n",
    "            parole_presenti.append(parola.text)\n",
    "        else:\n",
    "            parole_assenti.append(parola.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05bfa0a6",
   "metadata": {},
   "source": [
    "### Stampiamo Risultati: Parole Presenti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de0cb10",
   "metadata": {},
   "outputs": [],
   "source": [
    "parole_presenti=(set(parole_presenti))\n",
    "print (parole_presenti)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf2e0ba",
   "metadata": {},
   "source": [
    "### Stampiamo Risultati: Parole Assenti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e65df79",
   "metadata": {},
   "outputs": [],
   "source": [
    "parole_assenti=(set(parole_assenti))\n",
    "print (parole_assenti)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
