{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5bb2981e-1bb3-409b-be69-cf2b1363c061",
   "metadata": {},
   "source": [
    "# Corpus File Preprocessing for Word2Vec:\n",
    "## 1. Load and read the corpus.\n",
    "## 2. Tokenize the text.\n",
    "## 3. Lowercase all words.\n",
    "## 4. Remove stop words and non-alphabetic words.\n",
    "## 5. (Optional) Lemmatize words.\n",
    "## 6. Optionally, subsample frequent words.\n",
    "## 7. Prepare the corpus for Word2Vec (list of tokenized sentences).\n",
    "## 8. Train the Word2Vec model using Gensim or other frameworks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ebb78b5-e7f4-42ce-ba79-518a749e317d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('punkt_tab')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import string\n",
    "\n",
    "import spacy\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed1d4f8-143e-4355-894e-7f23707a0ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r\"/home/acarugat/MyRepo1/Testi/Manzoni/Corpus.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    corpus = file.read()\n",
    "corpus=sent_tokenize(corpus, language='italian')\n",
    "print (corpus[1:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86343135-939a-4c3b-b913-949f1784fba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_corpus = [word_tokenize(sentence.lower(), language='italian') for sentence in corpus]\n",
    "\n",
    "punteggiatura = string.punctuation+\"«»’\"\n",
    "\n",
    "print (punteggiatura)\n",
    "\n",
    "tokenized_corpus2=[]\n",
    "for sentence in tokenized_corpus:\n",
    "    sentence2=[]\n",
    "    for word in sentence:\n",
    "        if word not in punteggiatura:\n",
    "            sentence2.append(word)\n",
    "    tokenized_corpus2.append(sentence2)\n",
    "    \n",
    "tokenized_corpus=tokenized_corpus2\n",
    "print (tokenized_corpus[0:10])      \n",
    "    \n",
    "#okenized_corpus = [word for word in tokenized_corpus if word not in string.punctuation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f4701f-b473-406a-8e24-945a2f9d963a",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('italian'))\n",
    "cleaned_corpus = [[word for word in sentence if word not in stop_words] for sentence in tokenized_corpus]\n",
    "print (cleaned_corpus[0:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18be764d-adff-4a4a-9a6b-c8b038ce6332",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaned_corpus = [[word for word in sentence if word.isalpha()] for sentence in cleaned_corpus]\n",
    "#print (cleaned_corpus[0:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b0e014-6bd4-4bee-9c7b-958ede9459cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "#lemmatized_corpus = [[lemmatizer.lemmatize(word) for word in sentence] for sentence in cleaned_corpus]\n",
    "#print (lemmatized_corpus[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f8fec5-41b3-4745-9d3f-ac244f263264",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Italian model\n",
    "#nlp = spacy.load(\"it_core_news_sm\")\n",
    "#lemmatized_corpus = []\n",
    "\n",
    "#for sentence in cleaned_corpus:\n",
    "    # Process each sentence with spaCy NLP pipeline\n",
    "#    doc = nlp(\" \".join(sentence))\n",
    "    # Extract lemmatized words\n",
    "#    lemmatized_sentence = [token.lemma_ for token in doc]\n",
    "#    lemmatized_corpus.append(lemmatized_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e6621b-8001-48b1-807a-487f8c221a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print (\"Original Corpus = \", corpus)\n",
    "#print (\"Tokenized Corpus = \", tokenized_corpus)\n",
    "#print (\"Cleaned Corpus = \", cleaned_corpus)\n",
    "#print (\"Lemmatized Corpus = \", lemmatized_corpus[0:99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89223425-c8ab-430a-978f-f7dad9974feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r\"/home/acarugat/MyRepo1/Testi/Manzoni/Corpus-PP.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "    for sentence in cleaned_corpus:\n",
    "        file.write(\" \".join(sentence) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6534d80-3430-4fc3-967f-84d0e1fdc3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the Word2Vec model\n",
    "model = Word2Vec(sentences=cleaned_corpus, vector_size=100, window=5, min_count=5, workers=4)\n",
    "\n",
    "# Save the trained model\n",
    "model.save(\"word2vec_model.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60bcd48-19a4-4ffa-a5ef-eef740f13e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (model.wv.get_vecattr(\"lago\", \"count\"))\n",
    "print (model.wv.most_similar(\"lago\"))\n",
    "print (model.wv.similarity(\"lago\", \"fiume\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b8f6e4-2c1e-4351-bdc5-68ab77c9fcd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the Word2Vec model with more epochs\n",
    "print (cleaned_corpus[10:100])\n",
    "model = Word2Vec(sentences=cleaned_corpus, vector_size=100, window=5, min_count=5, workers=4, epochs=100, sg=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0421d541-ede2-46c5-82d4-2845490f23af",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (model.wv.get_vecattr(\"lago\", \"count\"))\n",
    "print (model.wv.most_similar(\"lago\"))\n",
    "print (model.wv.similarity(\"lago\", \"fiume\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f0c885-2d68-4e69-851c-6e26d2d26098",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the Word2Vec model with more epochs and Skip-Grammar\n",
    "model = Word2Vec(sentences=cleaned_corpus, vector_size=100, window=5, min_count=5, workers=4, epochs=100, sg=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ef2fbe-c74a-4856-b1fa-6e2e86b9b876",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (model.wv.get_vecattr(\"lago\", \"count\"))\n",
    "print (model.wv.most_similar(\"lago\"))\n",
    "print (model.wv.similarity(\"lago\", \"fiume\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd17763-4df1-4ca9-8ee6-14efcfcc1198",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "\n",
    "# Flatten the list\n",
    "flattened_corpus = list(chain.from_iterable(cleaned_corpus))\n",
    "\n",
    "# Get unique tokens\n",
    "parole_presenti = list(set(flattened_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13476db2-0d40-4d8c-9802-213ecd1b41e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#In Gensim's Word2Vec, the vocabulary already contains unique words\n",
    "tokens_unici= list(model.wv.index_to_key)\n",
    "print (tokens_unici[1:10])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92689df9-5864-44cb-873d-8db30493254f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tabella = []\n",
    "for parola in tokens_unici:\n",
    "    print (parola)\n",
    "    c=model.wv.get_vecattr(parola,\"count\")\n",
    "    tabella.append({\"Parola\":parola, \"Frequenza\":c})\n",
    "df=pd.DataFrame(tabella)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c89f602-8fac-477f-ae4b-453a1eec5d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_dist=nltk.FreqDist(flattened_corpus)\n",
    "freq_dist.plot(10, cumulative=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa5198aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9785557014149964\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "def calcola_similarita(a, b): \n",
    "    tfidf = vectorizer.fit_transform([a, b])\n",
    "    return ((tfidf * tfidf.T).toarray())[0,1]\n",
    "\n",
    "file1_path = r\"/home/acarugat/MyRepo1/Testi/Manzoni/Fermo-e-Lucia.txt\"\n",
    "\n",
    "with open(file1_path, 'r', encoding='utf-8') as file: \n",
    "    testo1 = file.read()\n",
    "\n",
    "file2_path = r\"/home/acarugat/MyRepo1/Testi/Manzoni/PromessiSposi-1840.txt\"\n",
    "\n",
    "with open(file2_path, 'r', encoding='utf-8') as file: \n",
    "    testo2 = file.read()\n",
    "\n",
    "s=calcola_similarita(testo1,testo2)\n",
    "\n",
    "print (s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04701dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Initialize lists to store filenames and contents\n",
    "filenames = []\n",
    "contents = []\n",
    "\n",
    "# Walk through the home directory and read all text files\n",
    "print (\"Dimmi che documenti vuoi analizzare: Manzoni Presidenti Giornali o Parlamento\")\n",
    "line = input()\n",
    "\n",
    "for root, dirs, files in os.walk(r\"/home/acarugat/MyRepo1/Testi/\"+line):\n",
    "    for file in files:\n",
    "        if file not in [\"Corpus.txt\", \"Corpus-PP.txt\", \"PromessiSposi-stopword.txt\", \"MattarellaFine2024-preprocessato.txt\"]:\n",
    "            print (file)\n",
    "            if file.endswith(\".txt\"):  # Check if the file is a text file\n",
    "                filepath = os.path.join(root, file)\n",
    "                try:\n",
    "                    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "                        filenames.append(filepath)  # Store the file path\n",
    "                        contents.append(f.read())  # Store the file content\n",
    "                except Exception as e:\n",
    "                    print(f\"Error reading {filepath}: {e}\")\n",
    "\n",
    "# Create the dataset (DataFrame)\n",
    "dataset = pd.DataFrame({\n",
    "    \"filename\": filenames,\n",
    "    \"content\": contents\n",
    "})\n",
    "\n",
    "# Display the dataset\n",
    "# print(dataset)\n",
    "\n",
    "M = np.zeros((dataset.shape[0], dataset.shape[0])) # creiamo una matrice 30x30 per contenere i risultati di testo_i con testo_j\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc86c1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "for i, row in tqdm(dataset.iterrows(), total=dataset.shape[0], desc='1st level'): # definiamo i\n",
    "    for j, next_row in dataset.iterrows(): # definiamo j\n",
    "        M[i, j] = calcola_similarita(row.content, next_row.content) # popoliamo la matrice con i risultati"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43970f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (M)\n",
    "labels=dataset.filename.str.split('/').str[5:].str[1]\n",
    "labels=labels.str.split('.').str[0]\n",
    "print (labels)\n",
    "similarity_df = pd.DataFrame(M, columns=labels, index=labels) # creiamo un dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040fa322",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "mask = np.triu(np.ones_like(similarity_df)) # applichiamo una maschera per rimuovere la parte superiore della heatmap\n",
    "\n",
    "# creiamo la visualizzazione\n",
    "plt.figure(figsize=(12, 12))\n",
    "sns.heatmap(\n",
    "\t\t\tsimilarity_df,\n",
    "\t\t\tsquare=True, \n",
    "\t\t\tannot=True, \n",
    "\t\t\trobust=True,\n",
    "\t\t\tfmt='.2f',\n",
    "\t\t\tannot_kws={'size': 7, 'fontweight': 'bold'},\n",
    "\t\t\tyticklabels=similarity_df.columns,\n",
    "\t\t\txticklabels=similarity_df.columns,\n",
    "\t\t\tcmap=\"YlGnBu\",\n",
    "            #mask=mask\n",
    "\t\t\tmask=None\n",
    ")\n",
    "\n",
    "plt.title('Heatmap delle similarità tra testi', fontdict={'fontsize': 24})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3f104f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
