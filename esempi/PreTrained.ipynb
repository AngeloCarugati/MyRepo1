{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e3c2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Percorso del modello pre-addestrato\n",
    "model_path = \"/home/acarugat/GoogleNews-vectors-negative300.bin\"\n",
    "\n",
    "# Carica il modello in formato binario\n",
    "model = KeyedVectors.load_word2vec_format(model_path, binary=True)\n",
    "model.save(\"google_news_word2vec.model\")\n",
    "\n",
    "c=model.get_vecattr(\"Triskaidekaphobia\",\"count\")\n",
    "print (c)\n",
    "\n",
    "# Verifica che il modello sia stato caricato\n",
    "#print(model['word'])  # Mostra il vettore per la parola 'word'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634f87b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Word2Vec.load(\"google_news_word2vec.model\")\n",
    "c=model.get_vecattr(\"mattino\",\"count\")\n",
    "print (c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a37bc3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.similarity('dog', 'cat'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf528f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# Sostituisci 'percorso_al_modello/W2V.kv' con il percorso effettivo al file scaricato\n",
    "word_vectors = KeyedVectors.load(\"/home/acarugat/W2V.kv\", mmap='r')\n",
    "c=word_vectors.get_vecattr(\"idiosincrasia\",\"count\")\n",
    "print (c)\n",
    "\n",
    "# Esempio: ottieni il vettore per la parola 'casa'\n",
    "vettore_casa = word_vectors['casa']\n",
    "\n",
    "#print(word_vectors['casa'])  # Mostra il vettore per la parola 'casa'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca75355c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "# Scarica le risorse necessarie di NLTK\n",
    "import nltk\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('stopwords')\n",
    "\n",
    "# Pre-processamento del testo\n",
    "def preprocess_text(text):\n",
    "    stop_words = set(stopwords.words('italian'))  # Stop words in italiano\n",
    "    tokens = word_tokenize(text.lower())         # Tokenizzazione\n",
    "    tokens = [word for word in tokens if word.isalnum()]  # Rimuovi punteggiatura\n",
    "    tokens = [word for word in tokens if word not in stop_words]  # Rimuovi stop words\n",
    "    return tokens\n",
    "\n",
    "# Testo da analizzare\n",
    "# Metodo con il contesto `with` (raccomandato)\n",
    "file_path = \"/home/acarugat/DiscorsoMattarellaFine2024.txt\"\n",
    "\n",
    "# Leggi il contenuto del file\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    testo = file.read()\n",
    "\n",
    "#testo = \"Il cane corre nel giardino e il gatto lo osserva.\"\n",
    "tokens = preprocess_text(testo)\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e0394a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carica il modello Word2Vec pre-addestrato\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "model = KeyedVectors.load(\"/home/acarugat/W2V.kv\", mmap='r')\n",
    "\n",
    "# Parole presenti nel vocabolario del modello\n",
    "tokens_unici = list(set(tokens))\n",
    "print (tokens_unici)\n",
    "parole_presenti = [word for word in tokens_unici if word in model.key_to_index]\n",
    "#print(f\"Parole presenti nel modello: {(parole_presenti)}\")\n",
    "for parola in parole_presenti:\n",
    "    c=word_vectors.get_vecattr(parola,\"count\")\n",
    "    if (c<1000):\n",
    "        print (parola, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f629b41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcola similarità tra due parole\n",
    "if 'pace' in model.key_to_index and 'serenità' in model.key_to_index:\n",
    "    similarita = model.similarity('pace', 'serenità')\n",
    "    print(f\"Similarità tra 'pace' e 'serenità': {similarita}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa6a2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "freq_dist = nltk.FreqDist(tokens)\n",
    "#for k,v in freq_dist.items():\n",
    "    #print(str(k) + ':' + str(v))\n",
    "\n",
    "# Infine, plottiamo i risultati ottenuti in un grafico per visualizzare quali sono gli argomenti più discussi all’interno della pagina\n",
    "freq_dist.plot(10, cumulative=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1d24b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Supponiamo di avere una lista di vettori (ottenuti dal modello Word2Vec)\n",
    "# Esempio: vettori = [model[word] for word in parole_presenti]\n",
    "vettori = [model[word] for word in parole_presenti]\n",
    "           \n",
    "# Converti la lista in un array NumPy\n",
    "vettori_array = np.array(vettori)\n",
    "\n",
    "# Applica t-SNE\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "vettori_ridotti = tsne.fit_transform(vettori_array)\n",
    "\n",
    "# Visualizza i risultati\n",
    "plt.scatter(vettori_ridotti[:, 0], vettori_ridotti[:, 1])\n",
    "for i, parola in enumerate(parole_presenti):\n",
    "    plt.text(vettori_ridotti[i, 0] + 0.01, vettori_ridotti[i, 1] + 0.01, parola, fontsize=9)\n",
    "plt.title(\"Visualizzazione t-SNE\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42c0743",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Imposta il numero di cluster\n",
    "num_clusters = 10\n",
    "\n",
    "# Esegui il clustering\n",
    "kmeans = KMeans(n_clusters=num_clusters, n_init='auto', random_state=42)\n",
    "kmeans = KMeans(n_clusters=8, init='k-means++', n_init='auto', max_iter=300, tol=0.0001, verbose=0, random_state=None, copy_x=True, algorithm='lloyd')\n",
    "kmeans.fit(vettori)\n",
    "\n",
    "# Ottieni i cluster\n",
    "clusters = kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e37a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Raggruppa le parole per cluster\n",
    "cluster_parole = {i: [] for i in range(num_clusters)}\n",
    "for parola, cluster in zip(parole_presenti, clusters):\n",
    "    cluster_parole[cluster].append(parola)\n",
    "\n",
    "# Visualizza i cluster\n",
    "for cluster, parole in cluster_parole.items():\n",
    "    print(f\"Cluster {cluster}: {parole}\")\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    for i, parola in enumerate(parole):\n",
    "        plt.scatter(vettori_ridotti[i, 0], vettori_ridotti[i, 1])\n",
    "        plt.annotate(parola, (vettori_ridotti[i, 0], vettori_ridotti[i, 1]))\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ca1cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "dati_ridotti = tsne.fit_transform(vettori_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a074dcb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (len(dati_ridotti))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e92316b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applica KMeans per creare cluster\n",
    "n_clusters = 10  # Numero di cluster\n",
    "kmeans = KMeans(n_clusters=n_clusters, n_init='auto', random_state=42)\n",
    "labels = kmeans.fit_predict(vettori_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f79b256",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crea un grafico scatter con colori basati sui cluster\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(dati_ridotti[:, 0], dati_ridotti[:, 1], c=labels, cmap='tab10', s=50, alpha=0.7)\n",
    "plt.colorbar(label='Cluster')\n",
    "plt.title(\"Visualizzazione dei Cluster\")\n",
    "plt.xlabel(\"Dimensione 1\")\n",
    "plt.ylabel(\"Dimensione 2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75c577b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ottieni i cluster\n",
    "clusters = kmeans.labels_\n",
    "# Raggruppa le parole per cluster\n",
    "cluster_parole = {i: [] for i in range(n_clusters)}\n",
    "for parola, cluster in zip(parole_presenti, clusters):\n",
    "    cluster_parole[cluster].append(parola)\n",
    "# Visualizza i cluster\n",
    "for cluster, parole in cluster_parole.items():\n",
    "    print(f\"Cluster {cluster}: {parole}\")\n",
    "    #plt.figure(figsize=(10, 10))\n",
    "    #for i, parola in enumerate(parole):\n",
    "    #    plt.scatter(vettori_ridotti[i, 0], vettori_ridotti[i, 1])\n",
    "    #    plt.annotate(parola, (vettori_ridotti[i, 0], vettori_ridotti[i, 1]))\n",
    "    #plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db91ea65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "\n",
    "# Scarica le risorse necessarie\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# 1. Preprocessamento: Tokenizza e rimuovi stop words\n",
    "stop_words = set(stopwords.words('italian'))\n",
    "parole = word_tokenize(testo.lower())\n",
    "parole_pulite = [word for word in parole if word.isalnum() and word not in stop_words]\n",
    "\n",
    "# 2. Conta la frequenza delle parole\n",
    "frequenze = Counter(parole_pulite)\n",
    "\n",
    "# 3. Ordina per frequenza decrescente\n",
    "frequenze_ordinate = frequenze.most_common()\n",
    "\n",
    "# 4. Stampa le parole ordinate\n",
    "print(\"Frequenze delle parole (ordinate):\")\n",
    "for parola, frequenza in frequenze_ordinate:\n",
    "    print(f\"{parola}: {frequenza}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a68640",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "# Scarica le risorse necessarie\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Tokenizza il testo in frasi\n",
    "frasi = nltk.sent_tokenize(testo, language=\"italian\")\n",
    "\n",
    "# Calcola le lunghezze delle frasi\n",
    "frasi_lunghezze = [(frase.strip(), len(nltk.word_tokenize(frase, language=\"italian\"))) for frase in frasi]\n",
    "\n",
    "# Ordina per lunghezza decrescente\n",
    "frasi_lunghezze.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Stampa le frasi ordinate\n",
    "print(\"Frasi ordinate per lunghezza decrescente:\")\n",
    "for frase, lunghezza in frasi_lunghezze:\n",
    "    print(f\"Lunghezza: {lunghezza} parole -> Frase: '{frase}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19bc3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from collections import Counter\n",
    "\n",
    "# Load the SpaCy Italian model\n",
    "nlp = spacy.load(\"it_core_news_md\")\n",
    "\n",
    "# Sample text\n",
    "text = \"Ciao, come stai? Ciao, oggi è una bella giornata. Come va oggi?\"\n",
    "\n",
    "# Process the text with SpaCy\n",
    "doc = nlp(text)\n",
    "\n",
    "# Create a Counter to count word frequencies\n",
    "word_freq = Counter()\n",
    "\n",
    "# Iterate over tokens and count only words (excluding punctuation, spaces, etc.)\n",
    "for token in doc:\n",
    "    if not token.is_punct and not token.is_space:\n",
    "        word_freq[token.text.lower()] += 1  # Convert to lowercase for case-insensitive counting\n",
    "\n",
    "# Print the word frequencies\n",
    "for word, freq in word_freq.items():\n",
    "    print(f\"{word}: {freq}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa5fb91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "# load the pretrained model\n",
    "model=fasttext.load_model(\"/home/acarugat/cc.it.300.bin\")\n",
    "\n",
    "# get nearest neighbors for the interested words (100 neighbors)\n",
    "arancia_nn=model.get_nearest_neighbors('arancia', k=100)\n",
    "kiwi_nn=model.get_nearest_neighbors('kiwi', k=100)\n",
    "\n",
    "# get only words sets (discard the similarity cosine)\n",
    "arancia_nn_words=set([el[1] for el in arancia_nn])\n",
    "kiwi_nn_words=set([el[1] for el in kiwi_nn])\n",
    "\n",
    "# compute the intersection\n",
    "common_similar_words=arancia_nn_words.intersection(kiwi_nn_words)\n",
    "\n",
    "print (common_similar_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99801ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Usa un modello pre-addestrato (es. GPT-3, GPT-4 o un transformer locale)\n",
    "text = \"Quando sono arrivato, la porta era chiusa. Ho pensato che fosse già troppo tardi.\"\n",
    "\n",
    "# Usa un LLM per classificare le subordinate\n",
    "classifier = pipeline(\"text-classification\", model=\"bert-base-multilingual-cased\")\n",
    "subordinate_types = classifier(text)\n",
    "print(subordinate_types)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f9c7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(subordinate_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0644239c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Carica il modello italiano\n",
    "nlp = spacy.load(\"it_core_news_md\")\n",
    "\n",
    "# Testo di esempio\n",
    "# text = \"Quando sono arrivato, la porta era chiusa. Ho pensato che fosse già troppo tardi.\"\n",
    "\n",
    "# Analisi del testo\n",
    "doc = nlp(testo)\n",
    "\n",
    "# Trova e classifica subordinate\n",
    "for sent in doc.sents:\n",
    "    print(f\"Frase: {sent.text}\")\n",
    "    for token in sent:\n",
    "        #print (token, token.dep_)\n",
    "        if token.dep_ in {\"advcl\", \"ccomp\", \"acl\"}:  # Tipi di subordinate\n",
    "            print(f\" - Subordinata trovata: '{token.text}' (Tipo: {token.dep_})\")\n",
    "\n",
    "sentences = doc.sents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b3089e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "text = \"\"\"\n",
    "La pizza è molto buona.\n",
    "Non mi piacciono le lumache.\n",
    "\"\"\"\n",
    "\n",
    "blob = TextBlob(text)\n",
    "blob.tags  # [('The', 'DT'), ('titular', 'JJ'),\n",
    "#  ('threat', 'NN'), ('of', 'IN'), ...]\n",
    "\n",
    "blob.noun_phrases  # WordList(['titular threat', 'blob',\n",
    "#            'ultimate movie monster',\n",
    "#            'amoeba-like mass', ...])\n",
    "\n",
    "for sentence in blob.sentences:\n",
    "    print(sentence, sentence.sentiment.polarity)\n",
    "    print (sentence.polarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1acb2da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load the frequencies from the file into a dictionary\n",
    "frequency_file = \"/home/acarugat/sorted.it.word.unigrams.utf8\"\n",
    "\n",
    "# Dictionary to store word frequencies\n",
    "word_frequencies = {}\n",
    "word_words = {}\n",
    "\n",
    "# Read the file and populate the dictionary\n",
    "with open(frequency_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        #print (line)\n",
    "        # Split the line into frequency and word\n",
    "        parts = line.strip().split()\n",
    "        if len(parts) == 2:  # Ensure the line has two elements\n",
    "            frequency, word = parts\n",
    "            word_frequencies[word] = int(frequency)\n",
    "            word_words[word] = word\n",
    "\n",
    "# Step 2: Analyze the text\n",
    "# text = \"ciao come stai oggi è una bella giornata\"\n",
    "\n",
    "# Tokenize the text (split into words)\n",
    "# words = text.split()\n",
    "\n",
    "# Retrieve frequencies for each word\n",
    "word_frequencies_in_text = {word: word_frequencies.get(word, 0) for word in parole_presenti}\n",
    "\n",
    "sorted_frequencies = sorted(word_frequencies_in_text.items(), key=lambda item: item[1], reverse=True)\n",
    "\n",
    "# Print the results\n",
    "for word, freq in sorted_frequencies:\n",
    "    if freq == 0:\n",
    "        print(\"Parola\", word, \"non presente nel dizionario\")\n",
    "    else:\n",
    "        print(f\"Word: {word}, Frequency: {freq}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81dd15a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"dbmdz/bert-base-italian-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4388881c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer.tokenize(testo)\n",
    "print (tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a0d7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from gensim.models import KeyedVectors\n",
    "import numpy as np\n",
    "\n",
    "# Carica un modello di embedding preaddestrato (es. FastText o Word2Vec)\n",
    "model_path = \"/home/acarugat/cc.it.300.vec\"  # Sostituisci con il percorso del modello\n",
    "word_vectors = KeyedVectors.load_word2vec_format(model_path, binary=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4012f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "words = parole_presenti\n",
    "print (words)\n",
    "\n",
    "# Ottieni i vettori per le parole selezionate\n",
    "word_embeddings = np.array([word_vectors[word] for word in words if word in word_vectors])\n",
    "\n",
    "# Definisci il numero di cluster\n",
    "k = 100\n",
    "\n",
    "# Applica KMeans\n",
    "kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "kmeans.fit(word_embeddings)\n",
    "\n",
    "# Associa ogni parola al cluster\n",
    "clusters = {word: kmeans.labels_[i] for i, word in enumerate(words)}\n",
    "\n",
    "# Stampa i risultati\n",
    "for cluster_id in range(k):\n",
    "    print(f\"Cluster {cluster_id}:\")\n",
    "    for word, label in clusters.items():\n",
    "        if label == cluster_id:\n",
    "            print(f\" - {word}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc0e3196",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('kings', 0.7138045430183411), ('queen', 0.6510956883430481), ('monarch', 0.6413194537162781), ('crown_prince', 0.6204220056533813), ('prince', 0.6159993410110474), ('sultan', 0.5864824056625366), ('ruler', 0.5797567367553711), ('princes', 0.5646552443504333), ('Prince_Paras', 0.5432944297790527), ('throne', 0.5422105193138123)]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "\n",
    "# Step 1: Load the pre-trained word vectors\n",
    "vectors = KeyedVectors.load_word2vec_format(\"GoogleNews-vectors-negative300.bin\", binary=True)\n",
    "\n",
    "# Step 2: Create a new Word2Vec model with the same vector size\n",
    "model = Word2Vec(vector_size=vectors.vector_size, min_count=1)\n",
    "\n",
    "# Step 3: Build a dummy vocabulary using the keys from the pre-trained vectors\n",
    "# (This step is required to initialize the model's vocabulary.)\n",
    "fake_corpus = [[word] for word in vectors.key_to_index.keys()]\n",
    "model.build_vocab(fake_corpus)\n",
    "\n",
    "# Step 4: Replace the vectors in the Word2Vec model with the pre-trained vectors\n",
    "model.wv.vectors = vectors.vectors\n",
    "model.wv.key_to_index = vectors.key_to_index\n",
    "model.wv.index_to_key = vectors.index_to_key\n",
    "\n",
    "# Step 5: Save the full Word2Vec model\n",
    "model.save(\"GoogleNews_word2vec_full.model\")\n",
    "\n",
    "# Step 6: Load the model to test\n",
    "loaded_model = Word2Vec.load(\"GoogleNews_word2vec_full.model\")\n",
    "print(loaded_model.wv.most_similar(\"king\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58654350",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Ciao', 0.8584988713264465), ('ciaoooo', 0.8168343305587769), ('ciaoo', 0.8079257011413574), ('ciaoooooo', 0.8062298893928528), ('ciaooooo', 0.799203634262085), ('ciaooo', 0.7907391786575317), ('grazie.ciao', 0.7903546094894409), ('ciaooooooo', 0.7891957759857178), ('ciaoooooooo', 0.788817286491394), ('ciaooooooooo', 0.7880434393882751)]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# Load the FastText Italian vectors (text format)\n",
    "it_vectors = KeyedVectors.load_word2vec_format(\"cc.it.300.vec\", binary=False)\n",
    "\n",
    "# Test similarity\n",
    "print(it_vectors.most_similar(\"ciao\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4381ea0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Step 3: Build a dummy vocabulary using the keys from the pre-trained vectors\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# (This step is required to initialize the model's vocabulary.)\u001b[39;00m\n\u001b[1;32m      3\u001b[0m fake_corpus \u001b[38;5;241m=\u001b[39m [[word] \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m it_vectors\u001b[38;5;241m.\u001b[39mkey_to_index\u001b[38;5;241m.\u001b[39mkeys()]\n\u001b[0;32m----> 4\u001b[0m model\u001b[38;5;241m.\u001b[39mbuild_vocab(fake_corpus)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Step 4: Replace the vectors in the Word2Vec model with the pre-trained vectors\u001b[39;00m\n\u001b[1;32m      7\u001b[0m model\u001b[38;5;241m.\u001b[39mwv\u001b[38;5;241m.\u001b[39mvectors \u001b[38;5;241m=\u001b[39m it_vectors\u001b[38;5;241m.\u001b[39mvectors\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# Step 3: Build a dummy vocabulary using the keys from the pre-trained vectors\n",
    "# (This step is required to initialize the model's vocabulary.)\n",
    "fake_corpus = [[word] for word in it_vectors.key_to_index.keys()]\n",
    "model.build_vocab(fake_corpus)\n",
    "\n",
    "# Step 4: Replace the vectors in the Word2Vec model with the pre-trained vectors\n",
    "model.wv.vectors = it_vectors.vectors\n",
    "model.wv.key_to_index = it_vectors.key_to_index\n",
    "model.wv.index_to_key = it_vectors.index_to_key\n",
    "\n",
    "# Step 5: Save the full Word2Vec model\n",
    "model.save(\"cc.it.300.vec.model\")\n",
    "\n",
    "# Step 6: Load the model to test\n",
    "loaded_model = Word2Vec.load(\"cc.it.300.vec.model\")\n",
    "print(loaded_model.wv.most_similar(\"ciao\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6935ec5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
